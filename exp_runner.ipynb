{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1741438245033,
     "user": {
      "displayName": "Camille Dubois",
      "userId": "14225654920681408553"
     },
     "user_tz": -60
    },
    "id": "rfbK97zj5e7_",
    "outputId": "8cd2adcc-51e1-4db2-b4c7-0553222c34f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in Neus_project: ['.git', '.gitignore', 'confs', 'custom_sampling.ipynb', 'data', 'download_data.sh', 'exp', 'exp_runner-F.ipynb', 'exp_runner.ipynb', 'exp_runner.py', 'LICENSE', 'models', 'play_with_data.ipynb', 'preprocess_custom_data', 'README.md', 'requirements.txt', 'rescale_images.ipynb', 'src', 'static']\n"
     ]
    }
   ],
   "source": [
    "# Check file access\n",
    "import os\n",
    "\n",
    "project_path = \"./\"\n",
    "files = os.listdir(project_path)\n",
    "\n",
    "print(\"Files in Neus_project:\", files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bp5qrBft4cOX"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "executionInfo": {
     "elapsed": 4047,
     "status": "error",
     "timestamp": 1741438252218,
     "user": {
      "displayName": "Camille Dubois",
      "userId": "14225654920681408553"
     },
     "user_tz": -60
    },
    "id": "FL5m5zYc4boW",
    "outputId": "a4932395-3091-415e-d108-ae1623ec17e0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from icecream import ic\n",
    "from scipy.spatial.transform import Rotation as Rot\n",
    "from scipy.spatial.transform import Slerp\n",
    "\n",
    "def load_K_Rt_from_P(filename, P=None):\n",
    "    if P is None:\n",
    "        lines = open(filename).read().splitlines()\n",
    "        if len(lines) == 4:\n",
    "            lines = lines[1:]\n",
    "        lines = [[x[0], x[1], x[2], x[3]] for x in (x.split(\" \") for x in lines)]\n",
    "        P = np.asarray(lines).astype(np.float32).squeeze()\n",
    "\n",
    "    out = cv.decomposeProjectionMatrix(P)\n",
    "    K = out[0]\n",
    "    R = out[1]\n",
    "    t = out[2]\n",
    "\n",
    "    K = K / K[2, 2]\n",
    "    intrinsics = np.eye(4)\n",
    "    intrinsics[:3, :3] = K\n",
    "\n",
    "    pose = np.eye(4, dtype=np.float32)\n",
    "    pose[:3, :3] = R.transpose()\n",
    "    pose[:3, 3] = (t[:3] / t[3])[:, 0]\n",
    "\n",
    "    return intrinsics, pose\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, conf):\n",
    "        super(Dataset, self).__init__()\n",
    "        print('Load data: Begin')\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.conf = conf\n",
    "\n",
    "        self.data_dir = conf.get_string('data_dir')\n",
    "        self.render_cameras_name = conf.get_string('render_cameras_name')\n",
    "        self.object_cameras_name = conf.get_string('object_cameras_name')\n",
    "\n",
    "        self.camera_outside_sphere = conf.get_bool('camera_outside_sphere', default=True)\n",
    "        self.scale_mat_scale = conf.get_float('scale_mat_scale', default=1.1)\n",
    "\n",
    "        camera_dict = np.load(os.path.join(self.data_dir, self.render_cameras_name))\n",
    "        self.camera_dict = camera_dict\n",
    "        self.images_lis = sorted(glob(os.path.join(self.data_dir, 'image/*.png')))\n",
    "        self.n_images = len(self.images_lis)\n",
    "        self.images_np = np.stack([cv.imread(im_name) for im_name in self.images_lis]) / 256.0\n",
    "        self.masks_lis = sorted(glob(os.path.join(self.data_dir, 'mask/*.png')))\n",
    "        self.masks_np = np.stack([cv.imread(im_name) for im_name in self.masks_lis]) / 256.0\n",
    "\n",
    "        self.world_mats_np = [camera_dict['world_mat_%d' % idx].astype(np.float32) for idx in range(self.n_images)]\n",
    "\n",
    "        self.scale_mats_np = []\n",
    "\n",
    "        self.scale_mats_np = [camera_dict['scale_mat_%d' % idx].astype(np.float32) for idx in range(self.n_images)]\n",
    "\n",
    "        self.intrinsics_all = []\n",
    "        self.pose_all = []\n",
    "\n",
    "        for scale_mat, world_mat in zip(self.scale_mats_np, self.world_mats_np):\n",
    "            P = world_mat @ scale_mat\n",
    "            P = P[:3, :4]\n",
    "            intrinsics, pose = load_K_Rt_from_P(None, P)\n",
    "            self.intrinsics_all.append(torch.from_numpy(intrinsics).float().to(self.device))\n",
    "            self.pose_all.append(torch.from_numpy(pose).float().to(self.device))\n",
    "\n",
    "        self.images = torch.from_numpy(self.images_np.astype(np.float32)).to(self.device)\n",
    "        self.masks  = torch.from_numpy(self.masks_np.astype(np.float32)).to(self.device)\n",
    "        self.intrinsics_all = torch.stack(self.intrinsics_all)\n",
    "        self.intrinsics_all_inv = torch.inverse(self.intrinsics_all)\n",
    "        self.focal = self.intrinsics_all[0][0, 0]\n",
    "        self.pose_all = torch.stack(self.pose_all)\n",
    "        self.H, self.W = self.images.shape[1], self.images.shape[2]\n",
    "        self.image_pixels = self.H * self.W\n",
    "\n",
    "        object_bbox_min = np.array([-1.01, -1.01, -1.01, 1.0])\n",
    "        object_bbox_max = np.array([ 1.01,  1.01,  1.01, 1.0])\n",
    "        object_scale_mat = np.load(os.path.join(self.data_dir, self.object_cameras_name))['scale_mat_0']\n",
    "        object_bbox_min = np.linalg.inv(self.scale_mats_np[0]) @ object_scale_mat @ object_bbox_min[:, None]\n",
    "        object_bbox_max = np.linalg.inv(self.scale_mats_np[0]) @ object_scale_mat @ object_bbox_max[:, None]\n",
    "        self.object_bbox_min = object_bbox_min[:3, 0]\n",
    "        self.object_bbox_max = object_bbox_max[:3, 0]\n",
    "\n",
    "        print('Load data: End')\n",
    "\n",
    "    def gen_rays_at(self, img_idx, resolution_level=1):\n",
    "        l = resolution_level\n",
    "        tx = torch.linspace(0, self.W - 1, self.W // l, device=self.device)\n",
    "        ty = torch.linspace(0, self.H - 1, self.H // l, device=self.device)\n",
    "        pixels_x, pixels_y = torch.meshgrid(tx, ty)\n",
    "        p = torch.stack([pixels_x, pixels_y, torch.ones_like(pixels_y)], dim=-1)\n",
    "        p = torch.matmul(self.intrinsics_all_inv[img_idx, None, None, :3, :3], p[:, :, :, None]).squeeze()\n",
    "        rays_v = p / torch.linalg.norm(p, ord=2, dim=-1, keepdim=True)\n",
    "        rays_v = torch.matmul(self.pose_all[img_idx, None, None, :3, :3], rays_v[:, :, :, None]).squeeze()\n",
    "        rays_o = self.pose_all[img_idx, None, None, :3, 3].expand(rays_v.shape)\n",
    "        return rays_o.transpose(0, 1), rays_v.transpose(0, 1)\n",
    "\n",
    "    def gen_random_rays_at(self, img_idx, batch_size):\n",
    "        pixels_x = torch.randint(low=0, high=self.W, size=[batch_size], device=self.device)\n",
    "        pixels_y = torch.randint(low=0, high=self.H, size=[batch_size], device=self.device)\n",
    "        color = self.images[img_idx][(pixels_y, pixels_x)]\n",
    "        mask = self.masks[img_idx][(pixels_y, pixels_x)]\n",
    "        p = torch.stack([pixels_x, pixels_y, torch.ones_like(pixels_y)], dim=-1).float()\n",
    "        p = torch.matmul(self.intrinsics_all_inv[img_idx, None, :3, :3], p[:, :, None]).squeeze()\n",
    "        rays_v = p / torch.linalg.norm(p, ord=2, dim=-1, keepdim=True)\n",
    "        rays_v = torch.matmul(self.pose_all[img_idx, None, :3, :3], rays_v[:, :, None]).squeeze()\n",
    "        rays_o = self.pose_all[img_idx, None, :3, 3].expand(rays_v.shape)\n",
    "\n",
    "        return torch.cat([rays_o, rays_v, color, mask[:, :1]], dim=-1)\n",
    "    \n",
    "    def gen_guided_rays_at(self, img_idx, prob_map, batch_size, Rx, Ry):\n",
    "        deltaX, deltaY = self.W // Rx, self.H // Ry\n",
    "        pixels = torch.multinomial(prob_map.view(-1), batch_size, replacement=True)\n",
    "        pixels_x_var = torch.randint(low=-deltaX // 2, high=deltaX // 2, size=[batch_size], device=self.device)\n",
    "        pixels_y_var = torch.randint(low=-deltaY // 2, high=deltaY // 2, size=[batch_size], device=self.device)\n",
    "        pixels_x = pixels % self.W + pixels_x_var\n",
    "        pixels_y = pixels // self.W + pixels_y_var\n",
    "        color = self.images[img_idx][(pixels_y, pixels_x)]\n",
    "        mask = self.masks[img_idx][(pixels_y, pixels_x)]\n",
    "        p = torch.stack([pixels_x, pixels_y, torch.ones_like(pixels_y)], dim=-1).float()\n",
    "        p = torch.matmul(self.intrinsics_all_inv[img_idx, None, :3, :3], p[:, :, None]).squeeze()\n",
    "        rays_v = p / torch.linalg.norm(p, ord=2, dim=-1, keepdim=True)\n",
    "        rays_v = torch.matmul(self.pose_all[img_idx, None, :3, :3], rays_v[:, :, None]).squeeze()\n",
    "        rays_o = self.pose_all[img_idx, None, :3, 3].expand(rays_v.shape)\n",
    "\n",
    "        return torch.cat([rays_o, rays_v, color, mask[:, :1]], dim=-1)\n",
    "    \n",
    "    def gen_rays_between(self, idx_0, idx_1, ratio, resolution_level=1):\n",
    "        l = resolution_level\n",
    "        tx = torch.linspace(0, self.W - 1, self.W // l, device=self.device)\n",
    "        ty = torch.linspace(0, self.H - 1, self.H // l, device=self.device)\n",
    "        pixels_x, pixels_y = torch.meshgrid(tx, ty)\n",
    "        p = torch.stack([pixels_x, pixels_y, torch.ones_like(pixels_y)], dim=-1)\n",
    "        p = torch.matmul(self.intrinsics_all_inv[0, None, None, :3, :3], p[:, :, :, None]).squeeze()\n",
    "        rays_v = p / torch.linalg.norm(p, ord=2, dim=-1, keepdim=True)\n",
    "        trans = self.pose_all[idx_0, :3, 3] * (1.0 - ratio) + self.pose_all[idx_1, :3, 3] * ratio\n",
    "        pose_0 = self.pose_all[idx_0].detach().cpu().numpy()\n",
    "        pose_1 = self.pose_all[idx_1].detach().cpu().numpy()\n",
    "        pose_0 = np.linalg.inv(pose_0)\n",
    "        pose_1 = np.linalg.inv(pose_1)\n",
    "        rot_0 = pose_0[:3, :3]\n",
    "        rot_1 = pose_1[:3, :3]\n",
    "        rots = Rot.from_matrix(np.stack([rot_0, rot_1]))\n",
    "        key_times = [0, 1]\n",
    "        slerp = Slerp(key_times, rots)\n",
    "        rot = slerp(ratio)\n",
    "        pose = np.diag([1.0, 1.0, 1.0, 1.0])\n",
    "        pose = pose.astype(np.float32)\n",
    "        pose[:3, :3] = rot.as_matrix()\n",
    "        pose[:3, 3] = ((1.0 - ratio) * pose_0 + ratio * pose_1)[:3, 3]\n",
    "        pose = np.linalg.inv(pose)\n",
    "        rot = torch.from_numpy(pose[:3, :3]).to(self.device)\n",
    "        trans = torch.from_numpy(pose[:3, 3]).to(self.device)\n",
    "        rays_v = torch.matmul(rot[None, None, :3, :3], rays_v[:, :, :, None]).squeeze()\n",
    "        rays_o = trans[None, None, :3].expand(rays_v.shape)\n",
    "        return rays_o.transpose(0, 1), rays_v.transpose(0, 1)\n",
    "\n",
    "    def near_far_from_sphere(self, rays_o, rays_d):\n",
    "        a = torch.sum(rays_d**2, dim=-1, keepdim=True)\n",
    "        b = 2.0 * torch.sum(rays_o * rays_d, dim=-1, keepdim=True)\n",
    "        mid = 0.5 * (-b) / a\n",
    "        near = mid - 1.0\n",
    "        far = mid + 1.0\n",
    "        return near, far\n",
    "\n",
    "    def image_at(self, idx, resolution_level):\n",
    "        img = cv.imread(self.images_lis[idx])\n",
    "        return (cv.resize(img, (self.W // resolution_level, self.H // resolution_level))).clip(0, 255)\n",
    "    \n",
    "    def get_world_scale_maps(self):\n",
    "        return self.world_mats_np, self.scale_mats_np\n",
    "    \n",
    "    def get_image_size(self):\n",
    "        return self.H, self.W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zloh9JiX4vCD"
   },
   "source": [
    "## Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QdpZENRM4w6K"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Positional encoding embedding. Code was taken from https://github.com/bmild/nerf.\n",
    "class Embedder:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        self.create_embedding_fn()\n",
    "\n",
    "    def create_embedding_fn(self):\n",
    "        embed_fns = []\n",
    "        d = self.kwargs['input_dims']\n",
    "        out_dim = 0\n",
    "        if self.kwargs['include_input']:\n",
    "            embed_fns.append(lambda x: x)\n",
    "            out_dim += d\n",
    "\n",
    "        max_freq = self.kwargs['max_freq_log2']\n",
    "        N_freqs = self.kwargs['num_freqs']\n",
    "\n",
    "        if self.kwargs['log_sampling']:\n",
    "            freq_bands = 2. ** torch.linspace(0., max_freq, N_freqs)\n",
    "        else:\n",
    "            freq_bands = torch.linspace(2.**0., 2.**max_freq, N_freqs)\n",
    "\n",
    "        for freq in freq_bands:\n",
    "            for p_fn in self.kwargs['periodic_fns']:\n",
    "                embed_fns.append(lambda x, p_fn=p_fn, freq=freq: p_fn(x * freq))\n",
    "                out_dim += d\n",
    "\n",
    "        self.embed_fns = embed_fns\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def embed(self, inputs):\n",
    "        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n",
    "\n",
    "\n",
    "def get_embedder(multires, input_dims=3):\n",
    "    embed_kwargs = {\n",
    "        'include_input': True,\n",
    "        'input_dims': input_dims,\n",
    "        'max_freq_log2': multires-1,\n",
    "        'num_freqs': multires,\n",
    "        'log_sampling': True,\n",
    "        'periodic_fns': [torch.sin, torch.cos],\n",
    "    }\n",
    "\n",
    "    embedder_obj = Embedder(**embed_kwargs)\n",
    "    def embed(x, eo=embedder_obj): return eo.embed(x)\n",
    "    return embed, embedder_obj.out_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsY8skNe4pIM"
   },
   "source": [
    "## Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cYw16y014qxN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#from models.embedder import get_embedder\n",
    "\n",
    "\n",
    "# This implementation is borrowed from IDR: https://github.com/lioryariv/idr\n",
    "class SDFNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_in,\n",
    "                 d_out,\n",
    "                 d_hidden,\n",
    "                 n_layers,\n",
    "                 skip_in=(4,),\n",
    "                 multires=0,\n",
    "                 bias=0.5,\n",
    "                 scale=1,\n",
    "                 geometric_init=True,\n",
    "                 weight_norm=True,\n",
    "                 inside_outside=False):\n",
    "        super(SDFNetwork, self).__init__()\n",
    "\n",
    "        dims = [d_in] + [d_hidden for _ in range(n_layers)] + [d_out]\n",
    "\n",
    "        self.embed_fn_fine = None\n",
    "\n",
    "        if multires > 0:\n",
    "            embed_fn, input_ch = get_embedder(multires, input_dims=d_in)\n",
    "            self.embed_fn_fine = embed_fn\n",
    "            dims[0] = input_ch\n",
    "\n",
    "        self.num_layers = len(dims)\n",
    "        self.skip_in = skip_in\n",
    "        self.scale = scale\n",
    "\n",
    "        for l in range(0, self.num_layers - 1):\n",
    "            if l + 1 in self.skip_in:\n",
    "                out_dim = dims[l + 1] - dims[0]\n",
    "            else:\n",
    "                out_dim = dims[l + 1]\n",
    "\n",
    "            lin = nn.Linear(dims[l], out_dim)\n",
    "\n",
    "            if geometric_init:\n",
    "                if l == self.num_layers - 2:\n",
    "                    if not inside_outside:\n",
    "                        torch.nn.init.normal_(lin.weight, mean=np.sqrt(np.pi) / np.sqrt(dims[l]), std=0.0001)\n",
    "                        torch.nn.init.constant_(lin.bias, -bias)\n",
    "                    else:\n",
    "                        torch.nn.init.normal_(lin.weight, mean=-np.sqrt(np.pi) / np.sqrt(dims[l]), std=0.0001)\n",
    "                        torch.nn.init.constant_(lin.bias, bias)\n",
    "                elif multires > 0 and l == 0:\n",
    "                    torch.nn.init.constant_(lin.bias, 0.0)\n",
    "                    torch.nn.init.constant_(lin.weight[:, 3:], 0.0)\n",
    "                    torch.nn.init.normal_(lin.weight[:, :3], 0.0, np.sqrt(2) / np.sqrt(out_dim))\n",
    "                elif multires > 0 and l in self.skip_in:\n",
    "                    torch.nn.init.constant_(lin.bias, 0.0)\n",
    "                    torch.nn.init.normal_(lin.weight, 0.0, np.sqrt(2) / np.sqrt(out_dim))\n",
    "                    torch.nn.init.constant_(lin.weight[:, -(dims[0] - 3):], 0.0)\n",
    "                else:\n",
    "                    torch.nn.init.constant_(lin.bias, 0.0)\n",
    "                    torch.nn.init.normal_(lin.weight, 0.0, np.sqrt(2) / np.sqrt(out_dim))\n",
    "\n",
    "            if weight_norm:\n",
    "                lin = nn.utils.weight_norm(lin)\n",
    "\n",
    "            setattr(self, \"lin\" + str(l), lin)\n",
    "\n",
    "        self.activation = nn.Softplus(beta=100)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs * self.scale\n",
    "        if self.embed_fn_fine is not None:\n",
    "            inputs = self.embed_fn_fine(inputs)\n",
    "\n",
    "        x = inputs\n",
    "        for l in range(0, self.num_layers - 1):\n",
    "            lin = getattr(self, \"lin\" + str(l))\n",
    "\n",
    "            if l in self.skip_in:\n",
    "                x = torch.cat([x, inputs], 1) / np.sqrt(2)\n",
    "\n",
    "            x = lin(x)\n",
    "\n",
    "            if l < self.num_layers - 2:\n",
    "                x = self.activation(x)\n",
    "        return torch.cat([x[:, :1] / self.scale, x[:, 1:]], dim=-1)\n",
    "\n",
    "    def sdf(self, x):\n",
    "        return self.forward(x)[:, :1]\n",
    "\n",
    "    def sdf_hidden_appearance(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def gradient(self, x):\n",
    "        x.requires_grad_(True)\n",
    "        y = self.sdf(x)\n",
    "        d_output = torch.ones_like(y, requires_grad=False, device=y.device)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=y,\n",
    "            inputs=x,\n",
    "            grad_outputs=d_output,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True)[0]\n",
    "        return gradients.unsqueeze(1)\n",
    "\n",
    "\n",
    "# This implementation is borrowed from IDR: https://github.com/lioryariv/idr\n",
    "class RenderingNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_feature,\n",
    "                 mode,\n",
    "                 d_in,\n",
    "                 d_out,\n",
    "                 d_hidden,\n",
    "                 n_layers,\n",
    "                 weight_norm=True,\n",
    "                 multires_view=0,\n",
    "                 squeeze_out=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "        self.squeeze_out = squeeze_out\n",
    "        dims = [d_in + d_feature] + [d_hidden for _ in range(n_layers)] + [d_out]\n",
    "\n",
    "        self.embedview_fn = None\n",
    "        if multires_view > 0:\n",
    "            embedview_fn, input_ch = get_embedder(multires_view)\n",
    "            self.embedview_fn = embedview_fn\n",
    "            dims[0] += (input_ch - 3)\n",
    "\n",
    "        self.num_layers = len(dims)\n",
    "\n",
    "        for l in range(0, self.num_layers - 1):\n",
    "            out_dim = dims[l + 1]\n",
    "            lin = nn.Linear(dims[l], out_dim)\n",
    "\n",
    "            if weight_norm:\n",
    "                lin = nn.utils.weight_norm(lin)\n",
    "\n",
    "            setattr(self, \"lin\" + str(l), lin)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, points, normals, view_dirs, feature_vectors):\n",
    "        if self.embedview_fn is not None:\n",
    "            view_dirs = self.embedview_fn(view_dirs)\n",
    "\n",
    "        rendering_input = None\n",
    "\n",
    "        if self.mode == 'idr':\n",
    "            rendering_input = torch.cat([points, view_dirs, normals, feature_vectors], dim=-1)\n",
    "        elif self.mode == 'no_view_dir':\n",
    "            rendering_input = torch.cat([points, normals, feature_vectors], dim=-1)\n",
    "        elif self.mode == 'no_normal':\n",
    "            rendering_input = torch.cat([points, view_dirs, feature_vectors], dim=-1)\n",
    "\n",
    "        x = rendering_input\n",
    "\n",
    "        for l in range(0, self.num_layers - 1):\n",
    "            lin = getattr(self, \"lin\" + str(l))\n",
    "\n",
    "            x = lin(x)\n",
    "\n",
    "            if l < self.num_layers - 2:\n",
    "                x = self.relu(x)\n",
    "\n",
    "        if self.squeeze_out:\n",
    "            x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# This implementation is borrowed from nerf-pytorch: https://github.com/yenchenlin/nerf-pytorch\n",
    "class NeRF(nn.Module):\n",
    "    def __init__(self,\n",
    "                 D=8,\n",
    "                 W=256,\n",
    "                 d_in=3,\n",
    "                 d_in_view=3,\n",
    "                 multires=0,\n",
    "                 multires_view=0,\n",
    "                 output_ch=4,\n",
    "                 skips=[4],\n",
    "                 use_viewdirs=False):\n",
    "        super(NeRF, self).__init__()\n",
    "        self.D = D\n",
    "        self.W = W\n",
    "        self.d_in = d_in\n",
    "        self.d_in_view = d_in_view\n",
    "        self.input_ch = 3\n",
    "        self.input_ch_view = 3\n",
    "        self.embed_fn = None\n",
    "        self.embed_fn_view = None\n",
    "\n",
    "        if multires > 0:\n",
    "            embed_fn, input_ch = get_embedder(multires, input_dims=d_in)\n",
    "            self.embed_fn = embed_fn\n",
    "            self.input_ch = input_ch\n",
    "\n",
    "        if multires_view > 0:\n",
    "            embed_fn_view, input_ch_view = get_embedder(multires_view, input_dims=d_in_view)\n",
    "            self.embed_fn_view = embed_fn_view\n",
    "            self.input_ch_view = input_ch_view\n",
    "\n",
    "        self.skips = skips\n",
    "        self.use_viewdirs = use_viewdirs\n",
    "\n",
    "        self.pts_linears = nn.ModuleList(\n",
    "            [nn.Linear(self.input_ch, W)] +\n",
    "            [nn.Linear(W, W) if i not in self.skips else nn.Linear(W + self.input_ch, W) for i in range(D - 1)])\n",
    "\n",
    "        ### Implementation according to the official code release\n",
    "        ### (https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py#L104-L105)\n",
    "        self.views_linears = nn.ModuleList([nn.Linear(self.input_ch_view + W, W // 2)])\n",
    "\n",
    "        ### Implementation according to the paper\n",
    "        # self.views_linears = nn.ModuleList(\n",
    "        #     [nn.Linear(input_ch_views + W, W//2)] + [nn.Linear(W//2, W//2) for i in range(D//2)])\n",
    "\n",
    "        if use_viewdirs:\n",
    "            self.feature_linear = nn.Linear(W, W)\n",
    "            self.alpha_linear = nn.Linear(W, 1)\n",
    "            self.rgb_linear = nn.Linear(W // 2, 3)\n",
    "        else:\n",
    "            self.output_linear = nn.Linear(W, output_ch)\n",
    "\n",
    "    def forward(self, input_pts, input_views):\n",
    "        if self.embed_fn is not None:\n",
    "            input_pts = self.embed_fn(input_pts)\n",
    "        if self.embed_fn_view is not None:\n",
    "            input_views = self.embed_fn_view(input_views)\n",
    "\n",
    "        h = input_pts\n",
    "        for i, l in enumerate(self.pts_linears):\n",
    "            h = self.pts_linears[i](h)\n",
    "            h = F.relu(h)\n",
    "            if i in self.skips:\n",
    "                h = torch.cat([input_pts, h], -1)\n",
    "\n",
    "        if self.use_viewdirs:\n",
    "            alpha = self.alpha_linear(h)\n",
    "            feature = self.feature_linear(h)\n",
    "            h = torch.cat([feature, input_views], -1)\n",
    "\n",
    "            for i, l in enumerate(self.views_linears):\n",
    "                h = self.views_linears[i](h)\n",
    "                h = F.relu(h)\n",
    "\n",
    "            rgb = self.rgb_linear(h)\n",
    "            return alpha, rgb\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "\n",
    "class SingleVarianceNetwork(nn.Module):\n",
    "    def __init__(self, init_val):\n",
    "        super(SingleVarianceNetwork, self).__init__()\n",
    "        self.register_parameter('variance', nn.Parameter(torch.tensor(init_val)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.ones([len(x), 1]) * torch.exp(self.variance * 10.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVHp2OgP47OT"
   },
   "source": [
    "## Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iw4SeEzG49JV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import logging\n",
    "import mcubes\n",
    "from icecream import ic\n",
    "\n",
    "\n",
    "def extract_fields(bound_min, bound_max, resolution, query_func):\n",
    "    N = 64\n",
    "    X = torch.linspace(bound_min[0], bound_max[0], resolution).split(N)\n",
    "    Y = torch.linspace(bound_min[1], bound_max[1], resolution).split(N)\n",
    "    Z = torch.linspace(bound_min[2], bound_max[2], resolution).split(N)\n",
    "\n",
    "    u = np.zeros([resolution, resolution, resolution], dtype=np.float32)\n",
    "    with torch.no_grad():\n",
    "        for xi, xs in enumerate(X):\n",
    "            for yi, ys in enumerate(Y):\n",
    "                for zi, zs in enumerate(Z):\n",
    "                    xx, yy, zz = torch.meshgrid(xs, ys, zs)\n",
    "                    pts = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1), zz.reshape(-1, 1)], dim=-1)\n",
    "                    val = query_func(pts).reshape(len(xs), len(ys), len(zs)).detach().cpu().numpy()\n",
    "                    u[xi * N: xi * N + len(xs), yi * N: yi * N + len(ys), zi * N: zi * N + len(zs)] = val\n",
    "    return u\n",
    "\n",
    "\n",
    "def extract_geometry(bound_min, bound_max, resolution, threshold, query_func):\n",
    "    print('threshold: {}'.format(threshold))\n",
    "    u = extract_fields(bound_min, bound_max, resolution, query_func)\n",
    "    vertices, triangles = mcubes.marching_cubes(u, threshold)\n",
    "    b_max_np = bound_max.detach().cpu().numpy()\n",
    "    b_min_np = bound_min.detach().cpu().numpy()\n",
    "\n",
    "    vertices = vertices / (resolution - 1.0) * (b_max_np - b_min_np)[None, :] + b_min_np[None, :]\n",
    "    return vertices, triangles\n",
    "\n",
    "\n",
    "def sample_pdf(bins, weights, n_samples, det=False):\n",
    "    # This implementation is from NeRF\n",
    "    # Get pdf\n",
    "    weights = weights + 1e-5  # prevent nans\n",
    "    pdf = weights / torch.sum(weights, -1, keepdim=True)\n",
    "    cdf = torch.cumsum(pdf, -1)\n",
    "    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)\n",
    "    # Take uniform samples\n",
    "    if det:\n",
    "        u = torch.linspace(0. + 0.5 / n_samples, 1. - 0.5 / n_samples, steps=n_samples)\n",
    "        u = u.expand(list(cdf.shape[:-1]) + [n_samples])\n",
    "    else:\n",
    "        u = torch.rand(list(cdf.shape[:-1]) + [n_samples])\n",
    "\n",
    "    # Invert CDF\n",
    "    u = u.contiguous()\n",
    "    inds = torch.searchsorted(cdf, u, right=True)\n",
    "    below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n",
    "    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n",
    "    inds_g = torch.stack([below, above], -1)  # (batch, N_samples, 2)\n",
    "\n",
    "    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n",
    "    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n",
    "    bins_g = torch.gather(bins.unsqueeze(1).expand(matched_shape), 2, inds_g)\n",
    "\n",
    "    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n",
    "    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
    "    t = (u - cdf_g[..., 0]) / denom\n",
    "    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "class NeuSRenderer:\n",
    "    def __init__(self,\n",
    "                 nerf,\n",
    "                 sdf_network,\n",
    "                 deviation_network,\n",
    "                 color_network,\n",
    "                 n_samples,\n",
    "                 n_importance,\n",
    "                 n_outside,\n",
    "                 up_sample_steps,\n",
    "                 perturb):\n",
    "        self.nerf = nerf\n",
    "        self.sdf_network = sdf_network\n",
    "        self.deviation_network = deviation_network\n",
    "        self.color_network = color_network\n",
    "        self.n_samples = n_samples\n",
    "        self.n_importance = n_importance\n",
    "        self.n_outside = n_outside\n",
    "        self.up_sample_steps = up_sample_steps\n",
    "        self.perturb = perturb\n",
    "\n",
    "    def render_core_outside(self, rays_o, rays_d, z_vals, sample_dist, nerf, background_rgb=None):\n",
    "        \"\"\"\n",
    "        Render background\n",
    "        \"\"\"\n",
    "        batch_size, n_samples = z_vals.shape\n",
    "\n",
    "        # Section length\n",
    "        dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "        dists = torch.cat([dists, torch.Tensor([sample_dist]).expand(dists[..., :1].shape).to(dists.device)], -1)\n",
    "        mid_z_vals = z_vals + dists * 0.5\n",
    "\n",
    "        # Section midpoints\n",
    "        pts = rays_o[:, None, :] + rays_d[:, None, :] * mid_z_vals[..., :, None]  # batch_size, n_samples, 3\n",
    "\n",
    "        dis_to_center = torch.linalg.norm(pts, ord=2, dim=-1, keepdim=True).clip(1.0, 1e10)\n",
    "        pts = torch.cat([pts / dis_to_center, 1.0 / dis_to_center], dim=-1)       # batch_size, n_samples, 4\n",
    "\n",
    "        dirs = rays_d[:, None, :].expand(batch_size, n_samples, 3)\n",
    "\n",
    "        pts = pts.reshape(-1, 3 + int(self.n_outside > 0))\n",
    "        dirs = dirs.reshape(-1, 3)\n",
    "\n",
    "        density, sampled_color = nerf(pts, dirs)\n",
    "        sampled_color = torch.sigmoid(sampled_color)\n",
    "        alpha = 1.0 - torch.exp(-F.softplus(density.reshape(batch_size, n_samples)) * dists)\n",
    "        alpha = alpha.reshape(batch_size, n_samples)\n",
    "        weights = alpha * torch.cumprod(torch.cat([torch.ones([batch_size, 1]), 1. - alpha + 1e-7], -1), -1)[:, :-1]\n",
    "        sampled_color = sampled_color.reshape(batch_size, n_samples, 3)\n",
    "        color = (weights[:, :, None] * sampled_color).sum(dim=1)\n",
    "        if background_rgb is not None:\n",
    "            color = color + background_rgb * (1.0 - weights.sum(dim=-1, keepdim=True))\n",
    "\n",
    "        return {\n",
    "            'color': color,\n",
    "            'sampled_color': sampled_color,\n",
    "            'alpha': alpha,\n",
    "            'weights': weights,\n",
    "        }\n",
    "\n",
    "    def up_sample(self, rays_o, rays_d, z_vals, sdf, n_importance, inv_s):\n",
    "        \"\"\"\n",
    "        Up sampling give a fixed inv_s\n",
    "        \"\"\"\n",
    "        batch_size, n_samples = z_vals.shape\n",
    "        pts = rays_o[:, None, :] + rays_d[:, None, :] * z_vals[..., :, None]  # n_rays, n_samples, 3\n",
    "        radius = torch.linalg.norm(pts, ord=2, dim=-1, keepdim=False)\n",
    "        inside_sphere = (radius[:, :-1] < 1.0) | (radius[:, 1:] < 1.0)\n",
    "        sdf = sdf.reshape(batch_size, n_samples)\n",
    "        prev_sdf, next_sdf = sdf[:, :-1], sdf[:, 1:]\n",
    "        prev_z_vals, next_z_vals = z_vals[:, :-1], z_vals[:, 1:]\n",
    "        mid_sdf = (prev_sdf + next_sdf) * 0.5\n",
    "        cos_val = (next_sdf - prev_sdf) / (next_z_vals - prev_z_vals + 1e-5)\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------\n",
    "        # Use min value of [ cos, prev_cos ]\n",
    "        # Though it makes the sampling (not rendering) a little bit biased, this strategy can make the sampling more\n",
    "        # robust when meeting situations like below:\n",
    "        #\n",
    "        # SDF\n",
    "        # ^\n",
    "        # |\\          -----x----...\n",
    "        # | \\        /\n",
    "        # |  x      x\n",
    "        # |---\\----/-------------> 0 level\n",
    "        # |    \\  /\n",
    "        # |     \\/\n",
    "        # |\n",
    "        # ----------------------------------------------------------------------------------------------------------\n",
    "        prev_cos_val = torch.cat([torch.zeros([batch_size, 1]), cos_val[:, :-1]], dim=-1)\n",
    "        cos_val = torch.stack([prev_cos_val, cos_val], dim=-1)\n",
    "        cos_val, _ = torch.min(cos_val, dim=-1, keepdim=False)\n",
    "        cos_val = cos_val.clip(-1e3, 0.0) * inside_sphere\n",
    "\n",
    "        dist = (next_z_vals - prev_z_vals)\n",
    "        prev_esti_sdf = mid_sdf - cos_val * dist * 0.5\n",
    "        next_esti_sdf = mid_sdf + cos_val * dist * 0.5\n",
    "        prev_cdf = torch.sigmoid(prev_esti_sdf * inv_s)\n",
    "        next_cdf = torch.sigmoid(next_esti_sdf * inv_s)\n",
    "        alpha = (prev_cdf - next_cdf + 1e-5) / (prev_cdf + 1e-5)\n",
    "        weights = alpha * torch.cumprod(\n",
    "            torch.cat([torch.ones([batch_size, 1]), 1. - alpha + 1e-7], -1), -1)[:, :-1]\n",
    "\n",
    "        z_samples = sample_pdf(z_vals, weights, n_importance, det=True).detach()\n",
    "        return z_samples\n",
    "\n",
    "    def cat_z_vals(self, rays_o, rays_d, z_vals, new_z_vals, sdf, last=False):\n",
    "        batch_size, n_samples = z_vals.shape\n",
    "        _, n_importance = new_z_vals.shape\n",
    "        pts = rays_o[:, None, :] + rays_d[:, None, :] * new_z_vals[..., :, None]\n",
    "        z_vals = torch.cat([z_vals, new_z_vals], dim=-1)\n",
    "        z_vals, index = torch.sort(z_vals, dim=-1)\n",
    "\n",
    "        if not last:\n",
    "            new_sdf = self.sdf_network.sdf(pts.reshape(-1, 3)).reshape(batch_size, n_importance)\n",
    "            sdf = torch.cat([sdf, new_sdf], dim=-1)\n",
    "            xx = torch.arange(batch_size)[:, None].expand(batch_size, n_samples + n_importance).reshape(-1)\n",
    "            index = index.reshape(-1)\n",
    "            sdf = sdf[(xx, index)].reshape(batch_size, n_samples + n_importance)\n",
    "\n",
    "        return z_vals, sdf\n",
    "\n",
    "    def render_core(self,\n",
    "                    rays_o,\n",
    "                    rays_d,\n",
    "                    z_vals,\n",
    "                    sample_dist,\n",
    "                    sdf_network,\n",
    "                    deviation_network,\n",
    "                    color_network,\n",
    "                    background_alpha=None,\n",
    "                    background_sampled_color=None,\n",
    "                    background_rgb=None,\n",
    "                    cos_anneal_ratio=0.0):\n",
    "        batch_size, n_samples = z_vals.shape\n",
    "\n",
    "        # Section length\n",
    "        dists = z_vals[..., 1:] - z_vals[..., :-1]\n",
    "        dists = torch.cat([dists, torch.Tensor([sample_dist]).expand(dists[..., :1].shape).to(dists.device)], -1)\n",
    "        mid_z_vals = z_vals + dists * 0.5\n",
    "\n",
    "        # Section midpoints\n",
    "        pts = rays_o[:, None, :] + rays_d[:, None, :] * mid_z_vals[..., :, None]  # n_rays, n_samples, 3\n",
    "        dirs = rays_d[:, None, :].expand(pts.shape)\n",
    "\n",
    "        pts = pts.reshape(-1, 3)\n",
    "        dirs = dirs.reshape(-1, 3)\n",
    "\n",
    "        sdf_nn_output = sdf_network(pts)\n",
    "        sdf = sdf_nn_output[:, :1]\n",
    "        feature_vector = sdf_nn_output[:, 1:]\n",
    "\n",
    "        gradients = sdf_network.gradient(pts).squeeze()\n",
    "        sampled_color = color_network(pts, gradients, dirs, feature_vector).reshape(batch_size, n_samples, 3)\n",
    "\n",
    "        inv_s = deviation_network(torch.zeros([1, 3]))[:, :1].clip(1e-6, 1e6)           # Single parameter\n",
    "        inv_s = inv_s.expand(batch_size * n_samples, 1)\n",
    "\n",
    "        true_cos = (dirs * gradients).sum(-1, keepdim=True)\n",
    "\n",
    "        # \"cos_anneal_ratio\" grows from 0 to 1 in the beginning training iterations. The anneal strategy below makes\n",
    "        # the cos value \"not dead\" at the beginning training iterations, for better convergence.\n",
    "        iter_cos = -(F.relu(-true_cos * 0.5 + 0.5) * (1.0 - cos_anneal_ratio) +\n",
    "                     F.relu(-true_cos) * cos_anneal_ratio)  # always non-positive\n",
    "\n",
    "        # Estimate signed distances at section points\n",
    "        estimated_next_sdf = sdf + iter_cos * dists.reshape(-1, 1) * 0.5\n",
    "        estimated_prev_sdf = sdf - iter_cos * dists.reshape(-1, 1) * 0.5\n",
    "\n",
    "        prev_cdf = torch.sigmoid(estimated_prev_sdf * inv_s)\n",
    "        next_cdf = torch.sigmoid(estimated_next_sdf * inv_s)\n",
    "\n",
    "        p = prev_cdf - next_cdf\n",
    "        c = prev_cdf\n",
    "\n",
    "        alpha = ((p + 1e-5) / (c + 1e-5)).reshape(batch_size, n_samples).clip(0.0, 1.0)\n",
    "\n",
    "        pts_norm = torch.linalg.norm(pts, ord=2, dim=-1, keepdim=True).reshape(batch_size, n_samples)\n",
    "        inside_sphere = (pts_norm < 1.0).float().detach()\n",
    "        relax_inside_sphere = (pts_norm < 1.2).float().detach()\n",
    "\n",
    "        # Render with background\n",
    "        if background_alpha is not None:\n",
    "            alpha = alpha * inside_sphere + background_alpha[:, :n_samples] * (1.0 - inside_sphere)\n",
    "            alpha = torch.cat([alpha, background_alpha[:, n_samples:]], dim=-1)\n",
    "            sampled_color = sampled_color * inside_sphere[:, :, None] +\\\n",
    "                            background_sampled_color[:, :n_samples] * (1.0 - inside_sphere)[:, :, None]\n",
    "            sampled_color = torch.cat([sampled_color, background_sampled_color[:, n_samples:]], dim=1)\n",
    "\n",
    "        weights = alpha * torch.cumprod(torch.cat([torch.ones([batch_size, 1]), 1. - alpha + 1e-7], -1), -1)[:, :-1]\n",
    "        weights_sum = weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        color = (sampled_color * weights[:, :, None]).sum(dim=1)\n",
    "        if background_rgb is not None:    # Fixed background, usually black\n",
    "            color = color + background_rgb * (1.0 - weights_sum)\n",
    "\n",
    "        # Eikonal loss\n",
    "        gradient_error = (torch.linalg.norm(gradients.reshape(batch_size, n_samples, 3), ord=2,\n",
    "                                            dim=-1) - 1.0) ** 2\n",
    "        gradient_error = (relax_inside_sphere * gradient_error).sum() / (relax_inside_sphere.sum() + 1e-5)\n",
    "\n",
    "        return {\n",
    "            'color': color,\n",
    "            'sdf': sdf,\n",
    "            'dists': dists,\n",
    "            'gradients': gradients.reshape(batch_size, n_samples, 3),\n",
    "            's_val': 1.0 / inv_s,\n",
    "            'mid_z_vals': mid_z_vals,\n",
    "            'weights': weights,\n",
    "            'cdf': c.reshape(batch_size, n_samples),\n",
    "            'gradient_error': gradient_error,\n",
    "            'inside_sphere': inside_sphere\n",
    "        }\n",
    "\n",
    "    def render(self, rays_o, rays_d, near, far, perturb_overwrite=-1, background_rgb=None, cos_anneal_ratio=0.0):\n",
    "        batch_size = len(rays_o)\n",
    "        sample_dist = 2.0 / self.n_samples   # Assuming the region of interest is a unit sphere\n",
    "        z_vals = torch.linspace(0.0, 1.0, self.n_samples)\n",
    "        z_vals = near + (far - near) * z_vals[None, :]\n",
    "\n",
    "        z_vals_outside = None\n",
    "        if self.n_outside > 0:\n",
    "            z_vals_outside = torch.linspace(1e-3, 1.0 - 1.0 / (self.n_outside + 1.0), self.n_outside)\n",
    "\n",
    "        n_samples = self.n_samples\n",
    "        perturb = self.perturb\n",
    "\n",
    "        if perturb_overwrite >= 0:\n",
    "            perturb = perturb_overwrite\n",
    "        if perturb > 0:\n",
    "            t_rand = (torch.rand([batch_size, 1]) - 0.5)\n",
    "            z_vals = z_vals + t_rand * 2.0 / self.n_samples\n",
    "\n",
    "            if self.n_outside > 0:\n",
    "                mids = .5 * (z_vals_outside[..., 1:] + z_vals_outside[..., :-1])\n",
    "                upper = torch.cat([mids, z_vals_outside[..., -1:]], -1)\n",
    "                lower = torch.cat([z_vals_outside[..., :1], mids], -1)\n",
    "                t_rand = torch.rand([batch_size, z_vals_outside.shape[-1]])\n",
    "                z_vals_outside = lower[None, :] + (upper - lower)[None, :] * t_rand\n",
    "\n",
    "        if self.n_outside > 0:\n",
    "            z_vals_outside = far / torch.flip(z_vals_outside, dims=[-1]) + 1.0 / self.n_samples\n",
    "\n",
    "        background_alpha = None\n",
    "        background_sampled_color = None\n",
    "\n",
    "        # Up sample\n",
    "        if self.n_importance > 0:\n",
    "            with torch.no_grad():\n",
    "                pts = rays_o[:, None, :] + rays_d[:, None, :] * z_vals[..., :, None]\n",
    "                sdf = self.sdf_network.sdf(pts.reshape(-1, 3)).reshape(batch_size, self.n_samples)\n",
    "\n",
    "                for i in range(self.up_sample_steps):\n",
    "                    new_z_vals = self.up_sample(rays_o,\n",
    "                                                rays_d,\n",
    "                                                z_vals,\n",
    "                                                sdf,\n",
    "                                                self.n_importance // self.up_sample_steps,\n",
    "                                                64 * 2**i)\n",
    "                    z_vals, sdf = self.cat_z_vals(rays_o,\n",
    "                                                  rays_d,\n",
    "                                                  z_vals,\n",
    "                                                  new_z_vals,\n",
    "                                                  sdf,\n",
    "                                                  last=(i + 1 == self.up_sample_steps))\n",
    "\n",
    "            n_samples = self.n_samples + self.n_importance\n",
    "\n",
    "        # Background model\n",
    "        if self.n_outside > 0:\n",
    "            z_vals_feed = torch.cat([z_vals, z_vals_outside], dim=-1)\n",
    "            z_vals_feed, _ = torch.sort(z_vals_feed, dim=-1)\n",
    "            ret_outside = self.render_core_outside(rays_o, rays_d, z_vals_feed, sample_dist, self.nerf)\n",
    "\n",
    "            background_sampled_color = ret_outside['sampled_color']\n",
    "            background_alpha = ret_outside['alpha']\n",
    "\n",
    "        # Render core\n",
    "        ret_fine = self.render_core(rays_o,\n",
    "                                    rays_d,\n",
    "                                    z_vals,\n",
    "                                    sample_dist,\n",
    "                                    self.sdf_network,\n",
    "                                    self.deviation_network,\n",
    "                                    self.color_network,\n",
    "                                    background_rgb=background_rgb,\n",
    "                                    background_alpha=background_alpha,\n",
    "                                    background_sampled_color=background_sampled_color,\n",
    "                                    cos_anneal_ratio=cos_anneal_ratio)\n",
    "\n",
    "        color_fine = ret_fine['color']\n",
    "        weights = ret_fine['weights']\n",
    "        weights_sum = weights.sum(dim=-1, keepdim=True)\n",
    "        gradients = ret_fine['gradients']\n",
    "        s_val = ret_fine['s_val'].reshape(batch_size, n_samples).mean(dim=-1, keepdim=True)\n",
    "\n",
    "        return {\n",
    "            'color_fine': color_fine,\n",
    "            's_val': s_val,\n",
    "            'cdf_fine': ret_fine['cdf'],\n",
    "            'weight_sum': weights_sum,\n",
    "            'weight_max': torch.max(weights, dim=-1, keepdim=True)[0],\n",
    "            'gradients': gradients,\n",
    "            'weights': weights,\n",
    "            'gradient_error': ret_fine['gradient_error'],\n",
    "            'inside_sphere': ret_fine['inside_sphere']\n",
    "        }\n",
    "\n",
    "    def extract_geometry(self, bound_min, bound_max, resolution, threshold=0.0):\n",
    "        return extract_geometry(bound_min,\n",
    "                                bound_max,\n",
    "                                resolution=resolution,\n",
    "                                threshold=threshold,\n",
    "                                query_func=lambda pts: -self.sdf_network.sdf(pts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBsBa7EZ5BOZ"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QjdiWJbp2ZEb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import trimesh\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from shutil import copyfile\n",
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "from pyhocon import ConfigFactory\n",
    "import sys\n",
    "\n",
    "#from models.dataset import Dataset\n",
    "#from models.fields import RenderingNetwork, SDFNetwork, SingleVarianceNetwork, NeRF\n",
    "#from models.renderer import NeuSRenderer\n",
    "\n",
    "\n",
    "class Runner:\n",
    "    def __init__(self, conf_path, mode='train', case='CASE_NAME', is_continue=False):\n",
    "        # self.device = torch.device('cuda')\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Configuration\n",
    "        self.conf_path = conf_path\n",
    "        f = open(self.conf_path)\n",
    "        conf_text = f.read()\n",
    "        conf_text = conf_text.replace('CASE_NAME', case)\n",
    "        f.close()\n",
    "\n",
    "        self.conf = ConfigFactory.parse_string(conf_text)\n",
    "        self.conf['dataset.data_dir'] = self.conf['dataset.data_dir'].replace('CASE_NAME', case)\n",
    "        self.base_exp_dir = self.conf['general.base_exp_dir']\n",
    "        os.makedirs(self.base_exp_dir, exist_ok=True)\n",
    "        self.dataset = Dataset(self.conf['dataset'])\n",
    "        self.iter_step = 0\n",
    "\n",
    "        # Training parameters\n",
    "        self.end_iter = self.conf.get_int('train.end_iter')\n",
    "        self.save_freq = self.conf.get_int('train.save_freq')\n",
    "        self.report_freq = self.conf.get_int('train.report_freq')\n",
    "        self.val_freq = self.conf.get_int('train.val_freq')\n",
    "        self.val_mesh_freq = self.conf.get_int('train.val_mesh_freq')\n",
    "        self.batch_size = self.conf.get_int('train.batch_size')\n",
    "        self.validate_resolution_level = self.conf.get_int('train.validate_resolution_level')\n",
    "        self.learning_rate = self.conf.get_float('train.learning_rate')\n",
    "        self.learning_rate_alpha = self.conf.get_float('train.learning_rate_alpha')\n",
    "        self.use_white_bkgd = self.conf.get_bool('train.use_white_bkgd')\n",
    "        self.warm_up_end = self.conf.get_float('train.warm_up_end', default=0.0)\n",
    "        self.anneal_end = self.conf.get_float('train.anneal_end', default=0.0)\n",
    "\n",
    "        # Weights\n",
    "        self.igr_weight = self.conf.get_float('train.igr_weight')\n",
    "        self.mask_weight = self.conf.get_float('train.mask_weight')\n",
    "        self.is_continue = is_continue\n",
    "        self.mode = mode\n",
    "        self.model_list = []\n",
    "        self.writer = None\n",
    "\n",
    "        # Networks\n",
    "        params_to_train = []\n",
    "        self.nerf_outside = NeRF(**self.conf['model.nerf']).to(self.device)\n",
    "        self.sdf_network = SDFNetwork(**self.conf['model.sdf_network']).to(self.device)\n",
    "        self.deviation_network = SingleVarianceNetwork(**self.conf['model.variance_network']).to(self.device)\n",
    "        self.color_network = RenderingNetwork(**self.conf['model.rendering_network']).to(self.device)\n",
    "        params_to_train += list(self.nerf_outside.parameters())\n",
    "        params_to_train += list(self.sdf_network.parameters())\n",
    "        params_to_train += list(self.deviation_network.parameters())\n",
    "        params_to_train += list(self.color_network.parameters())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)\n",
    "\n",
    "        self.renderer = NeuSRenderer(self.nerf_outside,\n",
    "                                     self.sdf_network,\n",
    "                                     self.deviation_network,\n",
    "                                     self.color_network,\n",
    "                                     **self.conf['model.neus_renderer'])\n",
    "\n",
    "        # Load checkpoint\n",
    "        latest_model_name = None\n",
    "        if is_continue:\n",
    "            model_list_raw = os.listdir(os.path.join(self.base_exp_dir, 'checkpoints'))\n",
    "            model_list = []\n",
    "            for model_name in model_list_raw:\n",
    "                if model_name[-3:] == 'pth' and int(model_name[5:-4]) <= self.end_iter:\n",
    "                    model_list.append(model_name)\n",
    "            model_list.sort()\n",
    "            latest_model_name = model_list[-1]\n",
    "\n",
    "        if latest_model_name is not None:\n",
    "            logging.info('Find checkpoint: {}'.format(latest_model_name))\n",
    "            self.load_checkpoint(latest_model_name)\n",
    "\n",
    "        # Backup codes and configs for debug\n",
    "        if self.mode[:5] == 'train':\n",
    "            self.file_backup()\n",
    "            \n",
    "        # Maps parameters   \n",
    "        self.guided_sampling = self.conf.get_bool('sampling.guided_sampling', default=False)\n",
    "        self.val_ps_freq = self.conf.get_int('sampling.val_ps_freq', default=5000)\n",
    "        if self.guided_sampling:\n",
    "            self.Rx = self.conf.get_int('sampling.resX', default=32)\n",
    "            self.Ry = self.conf.get_int('sampling.resY', default=32)\n",
    "            self.Rz = self.conf.get_int('sampling.resZ', default=32)\n",
    "            self.F = self.conf.get_int('sampling.factF', default=2)\n",
    "            \n",
    "            self.world_mats, self.scale_mats = self.dataset.get_world_scale_maps()\n",
    "            self.H, self.W = self.dataset.get_image_size()\n",
    "            self.maps = self.compute_maps()\n",
    "\n",
    "    def train(self):\n",
    "        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))\n",
    "        self.update_learning_rate()\n",
    "        res_step = self.end_iter - self.iter_step\n",
    "        image_perm = self.get_image_perm()\n",
    "\n",
    "        for iter_i in tqdm(range(res_step)):\n",
    "            if self.guided_sampling:\n",
    "                data1 = self.dataset.gen_random_rays_at(image_perm[self.iter_step % len(image_perm)], self.batch_size // 2)\n",
    "                data2 = self.dataset.gen_guided_rays_at(image_perm[(self.iter_step + 1) % len(image_perm)], self.maps[image_perm[(self.iter_step + 1) % len(image_perm)]], self.batch_size // 2, self.Rx, self.Ry)\n",
    "                data = torch.concatenate([data1, data2], axis=0)\n",
    "            \n",
    "            else:\n",
    "                data = self.dataset.gen_random_rays_at(image_perm[self.iter_step % len(image_perm)], self.batch_size)\n",
    "            \n",
    "            rays_o, rays_d, true_rgb, mask = data[:, :3], data[:, 3: 6], data[:, 6: 9], data[:, 9: 10]\n",
    "            near, far = self.dataset.near_far_from_sphere(rays_o, rays_d)\n",
    "\n",
    "            background_rgb = None\n",
    "            if self.use_white_bkgd:\n",
    "                background_rgb = torch.ones([1, 3])\n",
    "\n",
    "            if self.mask_weight > 0.0:\n",
    "                mask = (mask > 0.5).float()\n",
    "            else:\n",
    "                mask = torch.ones_like(mask)\n",
    "\n",
    "            mask_sum = mask.sum() + 1e-5\n",
    "            render_out = self.renderer.render(rays_o, rays_d, near, far,\n",
    "                                              background_rgb=background_rgb,\n",
    "                                              cos_anneal_ratio=self.get_cos_anneal_ratio())\n",
    "\n",
    "            color_fine = render_out['color_fine']\n",
    "            s_val = render_out['s_val']\n",
    "            cdf_fine = render_out['cdf_fine']\n",
    "            gradient_error = render_out['gradient_error']\n",
    "            weight_max = render_out['weight_max']\n",
    "            weight_sum = render_out['weight_sum']\n",
    "\n",
    "            # Loss\n",
    "            color_error = (color_fine - true_rgb) * mask\n",
    "            color_fine_loss = F.l1_loss(color_error, torch.zeros_like(color_error), reduction='sum') / mask_sum\n",
    "            psnr = 20.0 * torch.log10(1.0 / (((color_fine - true_rgb)**2 * mask).sum() / (mask_sum * 3.0)).sqrt())\n",
    "\n",
    "            eikonal_loss = gradient_error\n",
    "\n",
    "            mask_loss = F.binary_cross_entropy(weight_sum.clip(1e-3, 1.0 - 1e-3), mask)\n",
    "\n",
    "            loss = color_fine_loss +\\\n",
    "                   eikonal_loss * self.igr_weight +\\\n",
    "                   mask_loss * self.mask_weight\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.iter_step += 1\n",
    "\n",
    "            self.writer.add_scalar('Loss/loss', loss, self.iter_step)\n",
    "            self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)\n",
    "            self.writer.add_scalar('Loss/eikonal_loss', eikonal_loss, self.iter_step)\n",
    "            self.writer.add_scalar('Statistics/s_val', s_val.mean(), self.iter_step)\n",
    "            self.writer.add_scalar('Statistics/cdf', (cdf_fine[:, :1] * mask).sum() / mask_sum, self.iter_step)\n",
    "            self.writer.add_scalar('Statistics/weight_max', (weight_max * mask).sum() / mask_sum, self.iter_step)\n",
    "            self.writer.add_scalar('Statistics/psnr', psnr, self.iter_step)\n",
    "\n",
    "            if self.iter_step % self.report_freq == 0:\n",
    "                print(self.base_exp_dir)\n",
    "                print('iter:{:8>d} loss = {} lr={}'.format(self.iter_step, loss, self.optimizer.param_groups[0]['lr']))\n",
    "\n",
    "            if self.iter_step % self.save_freq == 0:\n",
    "                self.save_checkpoint()\n",
    "\n",
    "            if self.iter_step % self.val_freq == 0:\n",
    "                self.validate_image()\n",
    "\n",
    "            if self.iter_step % self.val_mesh_freq == 0:\n",
    "                self.validate_mesh()\n",
    "            \n",
    "            if self.guided_sampling & (self.iter_step % self.val_ps_freq == 0):\n",
    "                self.maps = self.compute_maps()\n",
    "\n",
    "            self.update_learning_rate()\n",
    "\n",
    "            if self.iter_step % len(image_perm) == 0:\n",
    "                image_perm = self.get_image_perm()\n",
    "\n",
    "    def get_image_perm(self):\n",
    "        return torch.randperm(self.dataset.n_images)\n",
    "\n",
    "    def get_cos_anneal_ratio(self):\n",
    "        if self.anneal_end == 0.0:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return np.min([1.0, self.iter_step / self.anneal_end])\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        if self.iter_step < self.warm_up_end:\n",
    "            learning_factor = self.iter_step / self.warm_up_end\n",
    "        else:\n",
    "            alpha = self.learning_rate_alpha\n",
    "            progress = (self.iter_step - self.warm_up_end) / (self.end_iter - self.warm_up_end)\n",
    "            learning_factor = (np.cos(np.pi * progress) + 1.0) * 0.5 * (1 - alpha) + alpha\n",
    "\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g['lr'] = self.learning_rate * learning_factor\n",
    "\n",
    "    def file_backup(self):\n",
    "        dir_lis = self.conf['general.recording']\n",
    "        os.makedirs(os.path.join(self.base_exp_dir, 'recording'), exist_ok=True)\n",
    "        for dir_name in dir_lis:\n",
    "            cur_dir = os.path.join(self.base_exp_dir, 'recording', dir_name)\n",
    "            os.makedirs(cur_dir, exist_ok=True)\n",
    "            files = os.listdir(dir_name)\n",
    "            for f_name in files:\n",
    "                if f_name[-3:] == '.py':\n",
    "                    copyfile(os.path.join(dir_name, f_name), os.path.join(cur_dir, f_name))\n",
    "\n",
    "        copyfile(self.conf_path, os.path.join(self.base_exp_dir, 'recording', 'config.conf'))\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_name):\n",
    "        checkpoint = torch.load(os.path.join(self.base_exp_dir, 'checkpoints', checkpoint_name), map_location=self.device)\n",
    "        self.nerf_outside.load_state_dict(checkpoint['nerf'])\n",
    "        self.sdf_network.load_state_dict(checkpoint['sdf_network_fine'])\n",
    "        self.deviation_network.load_state_dict(checkpoint['variance_network_fine'])\n",
    "        self.color_network.load_state_dict(checkpoint['color_network_fine'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.iter_step = checkpoint['iter_step']\n",
    "\n",
    "        logging.info('End')\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        checkpoint = {\n",
    "            'nerf': self.nerf_outside.state_dict(),\n",
    "            'sdf_network_fine': self.sdf_network.state_dict(),\n",
    "            'variance_network_fine': self.deviation_network.state_dict(),\n",
    "            'color_network_fine': self.color_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'iter_step': self.iter_step,\n",
    "        }\n",
    "\n",
    "        os.makedirs(os.path.join(self.base_exp_dir, 'checkpoints'), exist_ok=True)\n",
    "        torch.save(checkpoint, os.path.join(self.base_exp_dir, 'checkpoints', 'ckpt_{:0>6d}.pth'.format(self.iter_step)))\n",
    "\n",
    "    def validate_image(self, idx=-1, resolution_level=-1):\n",
    "        if idx < 0:\n",
    "            idx = np.random.randint(self.dataset.n_images)\n",
    "\n",
    "        print('Validate: iter: {}, camera: {}'.format(self.iter_step, idx))\n",
    "\n",
    "        if resolution_level < 0:\n",
    "            resolution_level = self.validate_resolution_level\n",
    "        rays_o, rays_d = self.dataset.gen_rays_at(idx, resolution_level=resolution_level)\n",
    "        H, W, _ = rays_o.shape\n",
    "        rays_o = rays_o.reshape(-1, 3).split(self.batch_size)\n",
    "        rays_d = rays_d.reshape(-1, 3).split(self.batch_size)\n",
    "\n",
    "        out_rgb_fine = []\n",
    "        out_normal_fine = []\n",
    "\n",
    "        for rays_o_batch, rays_d_batch in zip(rays_o, rays_d):\n",
    "            near, far = self.dataset.near_far_from_sphere(rays_o_batch, rays_d_batch)\n",
    "            background_rgb = torch.ones([1, 3]) if self.use_white_bkgd else None\n",
    "\n",
    "            render_out = self.renderer.render(rays_o_batch,\n",
    "                                              rays_d_batch,\n",
    "                                              near,\n",
    "                                              far,\n",
    "                                              cos_anneal_ratio=self.get_cos_anneal_ratio(),\n",
    "                                              background_rgb=background_rgb)\n",
    "\n",
    "            def feasible(key): return (key in render_out) and (render_out[key] is not None)\n",
    "\n",
    "            if feasible('color_fine'):\n",
    "                out_rgb_fine.append(render_out['color_fine'].detach().cpu().numpy())\n",
    "            if feasible('gradients') and feasible('weights'):\n",
    "                n_samples = self.renderer.n_samples + self.renderer.n_importance\n",
    "                normals = render_out['gradients'] * render_out['weights'][:, :n_samples, None]\n",
    "                if feasible('inside_sphere'):\n",
    "                    normals = normals * render_out['inside_sphere'][..., None]\n",
    "                normals = normals.sum(dim=1).detach().cpu().numpy()\n",
    "                out_normal_fine.append(normals)\n",
    "            del render_out\n",
    "\n",
    "        img_fine = None\n",
    "        if len(out_rgb_fine) > 0:\n",
    "            img_fine = (np.concatenate(out_rgb_fine, axis=0).reshape([H, W, 3, -1]) * 256).clip(0, 255)\n",
    "\n",
    "        normal_img = None\n",
    "        if len(out_normal_fine) > 0:\n",
    "            normal_img = np.concatenate(out_normal_fine, axis=0)\n",
    "            rot = np.linalg.inv(self.dataset.pose_all[idx, :3, :3].detach().cpu().numpy())\n",
    "            normal_img = (np.matmul(rot[None, :, :], normal_img[:, :, None])\n",
    "                          .reshape([H, W, 3, -1]) * 128 + 128).clip(0, 255)\n",
    "\n",
    "        os.makedirs(os.path.join(self.base_exp_dir, 'validations_fine'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.base_exp_dir, 'normals'), exist_ok=True)\n",
    "\n",
    "        for i in range(img_fine.shape[-1]):\n",
    "            if len(out_rgb_fine) > 0:\n",
    "                cv.imwrite(os.path.join(self.base_exp_dir,\n",
    "                                        'validations_fine',\n",
    "                                        '{:0>8d}_{}_{}.png'.format(self.iter_step, i, idx)),\n",
    "                           np.concatenate([img_fine[..., i],\n",
    "                                           self.dataset.image_at(idx, resolution_level=resolution_level)]))\n",
    "            if len(out_normal_fine) > 0:\n",
    "                cv.imwrite(os.path.join(self.base_exp_dir,\n",
    "                                        'normals',\n",
    "                                        '{:0>8d}_{}_{}.png'.format(self.iter_step, i, idx)),\n",
    "                           normal_img[..., i])\n",
    "\n",
    "    def render_novel_image(self, idx_0, idx_1, ratio, resolution_level):\n",
    "        \"\"\"\n",
    "        Interpolate view between two cameras.\n",
    "        \"\"\"\n",
    "        rays_o, rays_d = self.dataset.gen_rays_between(idx_0, idx_1, ratio, resolution_level=resolution_level)\n",
    "        H, W, _ = rays_o.shape\n",
    "        rays_o = rays_o.reshape(-1, 3).split(self.batch_size)\n",
    "        rays_d = rays_d.reshape(-1, 3).split(self.batch_size)\n",
    "\n",
    "        out_rgb_fine = []\n",
    "        for rays_o_batch, rays_d_batch in zip(rays_o, rays_d):\n",
    "            near, far = self.dataset.near_far_from_sphere(rays_o_batch, rays_d_batch)\n",
    "            background_rgb = torch.ones([1, 3]) if self.use_white_bkgd else None\n",
    "\n",
    "            render_out = self.renderer.render(rays_o_batch,\n",
    "                                              rays_d_batch,\n",
    "                                              near,\n",
    "                                              far,\n",
    "                                              cos_anneal_ratio=self.get_cos_anneal_ratio(),\n",
    "                                              background_rgb=background_rgb)\n",
    "\n",
    "            out_rgb_fine.append(render_out['color_fine'].detach().cpu().numpy())\n",
    "\n",
    "            del render_out\n",
    "\n",
    "        img_fine = (np.concatenate(out_rgb_fine, axis=0).reshape([H, W, 3]) * 256).clip(0, 255).astype(np.uint8)\n",
    "        return img_fine\n",
    "\n",
    "    def validate_mesh(self, world_space=False, resolution=64, threshold=0.0):\n",
    "        bound_min = torch.tensor(self.dataset.object_bbox_min, dtype=torch.float32)\n",
    "        bound_max = torch.tensor(self.dataset.object_bbox_max, dtype=torch.float32)\n",
    "\n",
    "        vertices, triangles =\\\n",
    "            self.renderer.extract_geometry(bound_min, bound_max, resolution=resolution, threshold=threshold)\n",
    "        os.makedirs(os.path.join(self.base_exp_dir, 'meshes'), exist_ok=True)\n",
    "\n",
    "        if world_space:\n",
    "            vertices = vertices * self.dataset.scale_mats_np[0][0, 0] + self.dataset.scale_mats_np[0][:3, 3][None]\n",
    "\n",
    "        mesh = trimesh.Trimesh(vertices, triangles)\n",
    "        mesh.export(os.path.join(self.base_exp_dir, 'meshes', '{:0>8d}.ply'.format(self.iter_step)))\n",
    "\n",
    "        logging.info('End')\n",
    "\n",
    "    def interpolate_view(self, img_idx_0, img_idx_1):\n",
    "        images = []\n",
    "        n_frames = 60\n",
    "        for i in range(n_frames):\n",
    "            print(i)\n",
    "            images.append(self.render_novel_image(img_idx_0,\n",
    "                                                  img_idx_1,\n",
    "                                                  np.sin(((i / n_frames) - 0.5) * np.pi) * 0.5 + 0.5,\n",
    "                          resolution_level=4))\n",
    "        for i in range(n_frames):\n",
    "            images.append(images[n_frames - i - 1])\n",
    "\n",
    "        fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
    "        video_dir = os.path.join(self.base_exp_dir, 'render')\n",
    "        os.makedirs(video_dir, exist_ok=True)\n",
    "        h, w, _ = images[0].shape\n",
    "        writer = cv.VideoWriter(os.path.join(video_dir,\n",
    "                                             '{:0>8d}_{}_{}.mp4'.format(self.iter_step, img_idx_0, img_idx_1)),\n",
    "                                fourcc, 30, (w, h))\n",
    "\n",
    "        for image in images:\n",
    "            writer.write(image)\n",
    "\n",
    "        writer.release()\n",
    "        \n",
    "    def compute_map(self, world_mat, scale_mat, W, H, s=1):\n",
    "\n",
    "        # Define geometric transformation\n",
    "        def h(x, P) :\n",
    "            x = torch.concat([x, torch.ones((x.shape[0], 1))], axis=1)\n",
    "            return P @ x.T\n",
    "\n",
    "        def g(x_hat) :\n",
    "            x_hat [:2, :] = x_hat[:2, :] / x_hat[2, :]\n",
    "            return x_hat\n",
    "\n",
    "        def f(x, P) :\n",
    "            return g(h(x, P))\n",
    "        \n",
    "        # Define logistic function\n",
    "        \n",
    "        def Phi(o, s) :\n",
    "            return s * torch.exp(-s * o) / (1 + torch.exp(-s * o)) \n",
    "        # Define projection matrix P\n",
    "        P = torch.Tensor(world_mat @ scale_mat)[:3, :4].to(self.device)\n",
    "\n",
    "        limX = 1\n",
    "        # Compute the grid of points in the world system\n",
    "        grid_Xx = torch.linspace(-limX, limX, self.Rx * self.F)\n",
    "        grid_Xy = torch.linspace(-limX, limX, self.Ry * self.F)\n",
    "        grid_Xz = torch.linspace(-limX, limX, self.Rz * self.F)\n",
    "\n",
    "        grid_Xx, grid_Xy, grid_Xz = torch.meshgrid(grid_Xx, grid_Xy, grid_Xz, indexing='xy')\n",
    "        grid_Xx = grid_Xx.reshape(-1, 1)\n",
    "        grid_Xy = grid_Xy.reshape(-1, 1)\n",
    "        grid_Xz = grid_Xz.reshape(-1, 1)\n",
    "\n",
    "        # Compute the grid of points in the camera system\n",
    "        grid_Ux = torch.linspace(0, W, self.Rx)\n",
    "        grid_Uy = torch.linspace(0, H, self.Ry)\n",
    "\n",
    "        distancex = 0.5 * (grid_Ux[1] - grid_Ux[0])\n",
    "        distancey = 0.5 * (grid_Uy[1] - grid_Uy[0])\n",
    "\n",
    "        grid_Ux, grid_Uy = torch.meshgrid(grid_Ux, grid_Uy, indexing='xy')\n",
    "        grid_Ux = grid_Ux.reshape(-1, 1)\n",
    "        grid_Uy = grid_Uy.reshape(-1, 1)\n",
    "        \n",
    "        # Compute the grid of points in the world-to-camera system\n",
    "        points_world = torch.cat([grid_Xx, grid_Xy, grid_Xz], dim=1)\n",
    "        grid_fXx, grid_fXy, grid_fXz = f(points_world, P)\n",
    "\n",
    "        # Compute the PDF of world points\n",
    "        with torch.no_grad():\n",
    "            points_world = points_world.to(self.device)\n",
    "            sdf_values = self.sdf_network.sdf(points_world)\n",
    "            pdf_values = Phi(sdf_values, s)\n",
    "            pdf_values = pdf_values.T\n",
    "        \n",
    "        # Compute the projected SDF in the camera system\n",
    "        Ux_min = grid_Ux - distancex\n",
    "        Ux_max = grid_Ux + distancex\n",
    "        Uy_min = grid_Uy - distancey\n",
    "        Uy_max = grid_Uy + distancey\n",
    "\n",
    "        UXx_intersect = (grid_fXx < Ux_max) & (grid_fXx > Ux_min) & (grid_fXx > 0)\n",
    "        UXy_intersect = (grid_fXy < Uy_max) & (grid_fXy > Uy_min) & (grid_fXy > 0)\n",
    "\n",
    "        mask = UXx_intersect & UXy_intersect\n",
    "        \n",
    "        probs = torch.sum(pdf_values * mask, axis=1, keepdims=True)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def compute_maps(self):\n",
    "        print('Computing guided sampling maps...')\n",
    "        maps = []\n",
    "        for i in range(len(self.world_mats)):\n",
    "            maps.append(self.compute_map(self.world_mats[i], self.scale_mats[i], self.H, self.W))\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f'Computing map {i}/{len(self.world_mats)}')\n",
    "                try:\n",
    "                    print(f'Norm diff: {torch.norm(self.maps[i] - maps[i]):.4f}')\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        return maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('Hello Wooden')\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Running on : {device}\")\n",
    "\n",
    "    FORMAT = \"[%(filename)s:%(lineno)s - %(funcName)20s() ] %(message)s\"\n",
    "    logging.basicConfig(level=logging.DEBUG, format=FORMAT)\n",
    "\n",
    "    # Filter out Jupyter-specific arguments\n",
    "    sys.argv = [arg for arg in sys.argv if not arg.startswith('--f=')]\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--conf', type=str, default='./confs/runs/project_DTU_small_tests-32_32_16.conf')\n",
    "    parser.add_argument('--mode', type=str, default='train')\n",
    "    parser.add_argument('--mcube_threshold', type=float, default=0.0)\n",
    "    parser.add_argument('--is_continue', default=False, action=\"store_true\")\n",
    "    parser.add_argument('--gpu', type=int, default=0)\n",
    "    parser.add_argument('--case', type=str, default='scan83')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Set default data type\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "\n",
    "    # Set default device\n",
    "    torch.set_default_device(device)\n",
    "\n",
    "    runner = Runner(args.conf, args.mode, args.case, args.is_continue)\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        runner.train()\n",
    "    elif args.mode == 'validate_mesh':\n",
    "        runner.validate_mesh(world_space=True, resolution=512, threshold=args.mcube_threshold)\n",
    "    elif args.mode.startswith('interpolate'):  # Interpolate views given two image indices\n",
    "        _, img_idx_0, img_idx_1 = args.mode.split('_')\n",
    "        img_idx_0 = int(img_idx_0)\n",
    "        img_idx_1 = int(img_idx_1)\n",
    "        runner.interpolate_view(img_idx_0, img_idx_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('Hello Wooden')\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Running on : {device}\")\n",
    "\n",
    "    FORMAT = \"[%(filename)s:%(lineno)s - %(funcName)20s() ] %(message)s\"\n",
    "    logging.basicConfig(level=logging.DEBUG, format=FORMAT)\n",
    "\n",
    "    # Filter out Jupyter-specific arguments\n",
    "    sys.argv = [arg for arg in sys.argv if not arg.startswith('--f=')]\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--conf', type=str, default='./confs/RUNS/project_DTU_small_tests-32-32-16.conf')\n",
    "    parser.add_argument('--mode', type=str, default='validate_mesh')\n",
    "    parser.add_argument('--mcube_threshold', type=float, default=0.0)\n",
    "    parser.add_argument('--is_continue', default=True, action=\"store_true\")\n",
    "    parser.add_argument('--gpu', type=int, default=0)\n",
    "    parser.add_argument('--case', type=str, default='case24')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Set default data type\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "\n",
    "    # Set default device\n",
    "    torch.set_default_device(device)\n",
    "\n",
    "    runner = Runner(args.conf, args.mode, args.case, args.is_continue)\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        runner.train()\n",
    "    elif args.mode == 'validate_mesh':\n",
    "        runner.validate_mesh(world_space=True, resolution=512, threshold=args.mcube_threshold)\n",
    "    elif args.mode.startswith('interpolate'):  # Interpolate views given two image indices\n",
    "        _, img_idx_0, img_idx_1 = args.mode.split('_')\n",
    "        img_idx_0 = int(img_idx_0)\n",
    "        img_idx_1 = int(img_idx_1)\n",
    "        runner.interpolate_view(img_idx_0, img_idx_1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
